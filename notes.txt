Specifications:
https://cyber.harvard.edu/rss/rss.html
https://www.rssboard.org/rss-specification


How to check if feed has changed:
https://stackoverflow.com/questions/1326398/detecting-new-new-items-in-a-rss-feed


TODO:

* Delete unused dependencies

*Add ability to check if add .rss to end of url works in url is invalid

*Don't allow similar duplicates ie https://www.reddit.com/r/cats/.rss vs https://www.reddit.com/r/cats.rss

*Set timeout for requesting feed url if it takes too long

*capatalization in url. Same url but it is seen as a different one

*JSON version of reddit feeds might have upvotes
	ex. http://www.reddit.com/r/inthemorning/.json

*cors

*make sure no route handlers can crash server

*infinite scrolling will fetch feeds twice initially for some reason. Might be due the last feed being visible and a race condition with the loading flag

*If it takes 12 minutes to update all feeds and feeds update every 10 minutes that will be an issue

*Delete posts after a certain date so the database doesn't grow too large

* When adding favorites to posts make sure not to delete posts that are favorited

* react query

*Feeds aren't being deleted when unsubscribing
		^
		| Probably related
		\/
* Received error:
	TypeError: Cannot read properties of undefined (reading 'feedid')
    at Object.getFeedId (/home/ariab/RSSReader/server/db/queries/feedQueries.js:11:24)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async getFeed (/home/ariab/RSSReader/server/controllers/feedController.js:12:24)

	Maybe make sure that when getting feedId, if subscriptionid doesn't exists handle that somehow
	
* change sidebar color to chatgpt sidebar color


*Adding new feed will always request feed and parse xml
	No need to do this. First check if feed already exists before doing this
	
*When A feed can't be added because it alreaady exists in that folder, don't remove the inputs

* When destructuring or exporting, if there are a lot of variables, move them to sepearte lines

*Renaming feed doesn't change the names of the post sources
	(Same feeds in different folders could possibly have different names)
	
*Worker thread when does it get removed

No need to pass down some props to SidebarButton if passing down the subscription object
	
* Add loading icon when adding feed

*If all feeds is already selected when deleting a feed, that feed will still show up in the Content
	If any feed/folder is selected, it won't be updated when deleting feeds since the state for selection isn't changing
	
* Handle cases where posts or feeds don't contain information in their xml. Such as missing titles, dates etc.

*Do atom feeds use the rss attribute in ompl files

* Add limits to the multer middleware to prevent too large of files being submitted

* Save session to db maybe?

* When importing from ompl, maybe create  new thread for getting posts from all the new feeds

* When importing from opml I think some feeds are taking really long because the url is invalid and they might be timing out

* Duplicate subscriptions in ompl folder are being added

* Currently if a subscription is added where the posts in that feed aren't able to be read. The subscription will still be added just without any posts (Do something about this)

* Add logging for when post data for a feed isn't able to be read.
	Log that feed so it can be tested later
	ex: https://www.wired.com/feed --posts unable to be read
	
* Should I connect to the db in server.js

* Possibly change the name feed to posts ie getFeed should become getPosts

* when clicking on feed, should pass feedid instead of subscriptionid since we are storing that in the selected object

* Create session secrets

* Feeds being requested too quickly results in 429 error
	-This could be an issue if different users all add the same feed in quick succession
	
* Using database transactions could allow me to run multiple test suites in parallel even if they use the same database

* remove allFeeds query param from frontend since it is no longer needed

* Uploading ompl currently uses real rss feeds. Probably make it use the test feeds

* When worker has an error, it looks like it never exits and prevents future jobs from running

	Worker for job "updatePosts" had an error {
	  err: Error: Unclosed root tag
	  Line: 2097
	  Column: 14
	  Char: 
		  at error (/home/ariab/RSSReader/server/node_modules/sax/lib/sax.js:651:10)
		  at strictFail (/home/ariab/RSSReader/server/node_modules/sax/lib/sax.js:677:7)
		  at end (/home/ariab/RSSReader/server/node_modules/sax/lib/sax.js:658:47)
		  at SAXParser.write (/home/ariab/RSSReader/server/node_modules/sax/lib/sax.js:975:14)
		  at SAXParser.close (/home/ariab/RSSReader/server/node_modules/sax/lib/sax.js:157:38)
		  at exports.Parser.Parser.parseString (/home/ariab/RSSReader/server/node_modules/rss-parser/node_modules/xml2js/lib/parser.js:327:42)
		  at Parser.parseString (/home/ariab/RSSReader/server/node_modules/rss-parser/node_modules/xml2js/lib/parser.js:5:59)
		  at /home/ariab/RSSReader/server/node_modules/rss-parser/lib/parser.js:33:22
		  at new Promise (<anonymous>)
		  at Parser.parseString (/home/ariab/RSSReader/server/node_modules/rss-parser/lib/parser.js:32:16)
	}
	Error: Job "updatePosts" is already running
		at Bree.run (/home/ariab/RSSReader/server/node_modules/bree/src/index.js:366:11)
		at /home/ariab/RSSReader/server/node_modules/bree/src/index.js:635:40
		at Timeout.scheduleTimeout [as _onTimeout] (/home/ariab/RSSReader/server/node_modules/@breejs/later/lib/index.js:1038:7)
		at listOnTimeout (node:internal/timers:569:17)
		at process.processTimers (node:internal/timers:512:7) undefined

* Comment from hacker news:
	I really like miniflux, but I really wish it had a weighted post ordering rather than a simple chronological ordering. I posted a feature request about this earlier this year[0], but the gist is this:
	If you subscribe to some feeds that post 100+ times a day (like a major news outlet) and others that post only once every couple months or so (like many personal blogs), you'll never catch the latter because those posts are always drowned in a sea of the former. Reddit deals with this problem by weighting posts from each subreddit you subscribe to so that your frontpage contains content from as many of your subs as possible.

	All I want for Christmas is for this idea to get some traction. Hoping to make the time to hack on this myself Q1 of next year.

	[0]: https://github.com/miniflux/v2/issues/1493

* Show the feed url for subscriptions

* When changing subscrition name key changes between renders. Is this bad?

* Currently updated the folder input with a useEffect with state dependencies. Idk if this is the correct way to do it

* Remove suggestions when entering in inputs

* Add tests to make sure that when posts are updated, posts that are already in db don't get added (Create test file for posts tests)

* Add db indexes

* Add ability to import opml into a specific folder. (Will have to flatten the existing folders in the file)

* Instead of using a worker thread for opml imports, use a job queue like BullMQ or pg-boss to make status bar easier as well as more scallable

* Bree can handle the case where if the job is already running it won't create another worker thread. Look at docs

* Add tests for updatePosts.js
	* Test for invalid xml, 404 pages, timeouts

* Validate params of importOPML and wrap in try catch or else server will crash

* Only let users submit .opml or .xml (or some other file extensions) and only let server accept files of those types

* When tables are truncated and browswer isn't refreshed. Click on a subscription. Error occurs on server. Should return 400 instead of 500

* When clicking  on subscriptions. The duration of the fetch alternations from ~20 ms to ~300 ms. IDK why

* Adjust timeouts for both updateFeedsPosts and getFeedHeaders

* rename feedContorller/getFeed to getPosts

* Instead of inserting each post individually you can insert all posts in a single query

* Every other request to the server seems to be slower for some reason

* Handle being rate limited. Maybe put rate limited urls into a set with their last checked time and wait like 30 minutes before updating posts from them again

* Sometimes a url will redirect. The origional url will be invalid but the redirect one will be valid
	ex: http://www.patentlyapple.com/patently-apple/rss.xml
	add redirect: 'follow' to fetch headers
	
* If a user uploads a feed that is really long (potentially thousands of posts) this will cause the server to lag since it isn't in it's own thread

* Some websites have rss links on them. Some readers are able to find these links from a given website url
	ex: https://appleworld.today/blog/
	
* Sometimes adding /rss to the end of the root of urls is the rss feed

* Test parsing of RDF feeds

* https://www.npmjs.com/package/@arn4v/feed-finder

TEST FEEDS:
404 with no rss feeds in html: https://www.gadgette.com/feed
Doesn't load /times out: http://www.asymco.com/feed/
HTML page that has rss feed in source: https://appleworld.today/blog/amp/?format=RSS
Error parsing: https://www.macupdate.com/mommy/updates.xml
HTML feed that doesn't have rss feed in source: https://www.tested.com/feeds/
redirects to valid feed: http://www.patentlyapple.com/patently-apple/rss.xml

* Cureate opml files:
https://github.com/plenaryapp/awesome-rss-feeds#with-category-and-without-category

* cant parse rdf feeds: 
http://rss.slashdot.org/Slashdot/slashdot

* Still need to add testing for updatePosts job

* saveSubscription might need to be executed in it's own thread since it couldn't potentially block with lots of parsing

* I may  be able to get away with simply partitioning the parsing of a single feed when a user adds a single subscription
https://nodejs.org/en/docs/guides/dont-block-the-event-loop
https://medium.com/dovetail-engineering/keep-the-node-js-event-loop-healthy-c3cae0e0e0f2

* I should be using a pool of workers probably
https://blog.logrocket.com/node-js-multithreading-worker-threads-why-they-matter/#:~:text=You%20can%20share%20memory%20with,are%20discussed%20in%20this%20article.
You can share memory with Worker threads and pass ArrayBuffer or SharedArrayBuffer objects that are specifically meant for that. Only use them if you need to do CPU-intensive tasks with large amounts of data. Some examples of CPU-intensive tasks with Node workers are discussed in this article. 

* Worker thread pool implementation:
https://www.npmjs.com/package/piscina

Importing opml pipeline:
1. Parse opml and return if error
2. Request each feed one by one from opml
3. Every time a response for a feed is received, queue it to a thread in a worker pool
4. Worker pool is solely responsible for parsing xml and returning js object
5. For each feed object returned, send this to the database to be added.

BullMQ going to production:
https://docs.bullmq.io/guide/going-to-production

Don't wait for jobs to complete:
https://blog.taskforce.sh/do-not-wait-for-your-jobs-to-complete/

* Currently bullmq is spawnding new processes since worker threads aren't working for some reason

* What happens when a user deletes a feed at the same time that that feed is already queued to be updated. Probably results in a database error since that feed no longer exists

* Read this:
https://roluquec.medium.com/job-queuing-101-start-using-bull-in-your-node-js-project-part-i-2be3ef36a42d

* Currently we are awaiting deleteUnsubscribedFeeds which isn't necessary since the http request doesn't rely on it

* What happens when a user adds a feed and then posts are updated. What if one of the posts has incoomplete properties like missing title. It looks like the feed and some posts will be added to the database and an error will be returned to the user. Maybe use transactions?

* When redis server stops, all jobs are deleted. Make sure to add jobs on server start

* When testing if jobs were added, make sure to clear redis before each test

* Somehow disable updateposts queue during tests

* I might be using UserError and QueryError wrong. They both shouldn't know about http so I could probably just use a single error instead of 2 seperate ones. Maybe just Error

* Should be using a seperate redis db for testing

* Workers are creating a new process for every job which is causing too many clients error. Also figure out a way to have every process use the same db pool